{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scFoundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Map RAW counts to samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8240/315372648.py:62: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html_content, 'html.parser')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html_content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Extract and print the data\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(item)\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36mextract_data\u001b[0;34m(soup)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Find all table rows in the table\u001b[39;00m\n\u001b[1;32m     24\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable table-bordered\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m---> 25\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows[\u001b[38;5;241m1\u001b[39m:]:  \u001b[38;5;66;03m# Skip the header row\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "\"\"\"Map RAW counts to samples\n",
    "\n",
    "Structure:\n",
    "    1. Imports, Variables, Functions\n",
    "    2. Load Data\n",
    "\"\"\"\n",
    "\n",
    "# 1. Imports, Variables, Functions\n",
    "# imports\n",
    "import numpy as np, pandas as pd, os, sys\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# variables\n",
    "html_content = \"../data/DiSignAtlas/dsa_diff_download.individual_downloads/DSA10289\"\n",
    "\n",
    "# functions\n",
    "\n",
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Control': ['GSM6943825', 'GSM6943826', 'GSM6943827', 'GSM6943828'], 'Case': ['GSM6943813', 'GSM6943814', 'GSM6943815', 'GSM6943816']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL\n",
    "url = 'http://www.inbirg.com/disignatlas/detail/DSA00009'\n",
    "\n",
    "# Send a GET request to fetch the HTML content\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Function to extract \"Control | Case\" information\n",
    "def extract_control_case_info(soup):\n",
    "    info = {}\n",
    "\n",
    "    # Find the table row with \"Control | Case\"\n",
    "    table_rows = soup.find_all('tr')\n",
    "    for row in table_rows:\n",
    "        cells = row.find_all('td')\n",
    "        if cells and 'Control | Case' in cells[0].get_text(strip=True):\n",
    "            if len(cells) >= 3:\n",
    "                control = cells[1].get_text(strip=True).split(';')\n",
    "                case = cells[2].get_text(strip=True).split(';')\n",
    "                info['Control'] = control\n",
    "                info['Case'] = case\n",
    "                break  # We found our row, so we can exit the loop\n",
    "\n",
    "    return info\n",
    "\n",
    "# Extract the \"Control | Case\" information\n",
    "control_case_info = extract_control_case_info(soup)\n",
    "\n",
    "# Print the extracted informationºº\n",
    "print(control_case_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "# Define function to extract \"Control | Case\" information\n",
    "def extract_control_case_info(soup):\n",
    "    info = {}\n",
    "\n",
    "    # Find the table row with \"Control | Case\"\n",
    "    table_rows = soup.find_all('tr')\n",
    "    for row in table_rows:\n",
    "        cells = row.find_all('td')\n",
    "        if cells and 'Control | Case' in cells[0].get_text(strip=True):\n",
    "            if len(cells) >= 3:\n",
    "                control = cells[1].get_text(strip=True).split(';')\n",
    "                case = cells[2].get_text(strip=True).split(';')\n",
    "                info['Control'] = control\n",
    "                info['Case'] = case\n",
    "                break  # We found our row, so we can exit the loop\n",
    "\n",
    "    return info\n",
    "\n",
    "# Function to download file and extract information\n",
    "def download_and_extract(dsaid):\n",
    "    output_path = os.path.join(\"..\", \"data\", \"DiSignAtlas\", \"dsa_diff_download.individual_downloads/\")\n",
    "    download_url = f\"http://www.inbirg.com/disignatlas/detail/{dsaid}\"\n",
    "    filename = f\"{dsaid}.html\"\n",
    "    file_path = os.path.join(output_path, filename)\n",
    "\n",
    "    max_retries = 10\n",
    "    timeout = 10\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(download_url, timeout=timeout)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, \"wb\") as file:\n",
    "                    file.write(response.content)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                info = extract_control_case_info(soup)\n",
    "                return info\n",
    "            else:\n",
    "                pass\n",
    "        except requests.RequestException as e:\n",
    "            pass\n",
    "\n",
    "    print(f\"Failed to download {dsaid} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Function to process dsaids in parallel\n",
    "def process_dsaids(dsaid_list):\n",
    "    with Pool(processes=8) as pool:  # Adjust the number of processes as needed\n",
    "        results = pool.map(download_and_extract, dsaid_list)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Load Identifiers\n",
    "data_path = os.path.join(\n",
    "    \"..\", \"data\", \"DiSignAtlas\", \"Disease_information_Datasets.csv\"\n",
    ")\n",
    "df = pd.read_csv(data_path)\n",
    "dsaids = df[\"dsaid\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48de0ea585f41a5951b34541cfb9dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download DSA02705 after 10 attempts.\n"
     ]
    }
   ],
   "source": [
    "extracted_info = process_map(download_and_extract, dsaids, max_workers=16, chunksize=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save results to a CSV file\n",
    "extracted_df = pd.DataFrame(extracted_info)\n",
    "extracted_df.to_csv('extracted_control_case_info.csv', index=False)\n",
    "print(\"Extraction and download complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_links_data_path = os.path.join(\n",
    "    \"..\", \"data\", \"DiSignAtlas\", \"external_links.pkl\"\n",
    ")\n",
    "\n",
    "import pickle\n",
    "\n",
    "a = pickle.load(open(external_links_data_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Map to Gene Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Store in Matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disease_sig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
