{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726775a5-c4b7-4fb4-a36d-ce151bbca008",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798f78aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere we will be creating an SQL database to store disease signatures from iLINCS!\\n\\nWe will filter those signatures which belong to diseases - which are >9,000 signatures\\nfrom the iLINCS database.\\n\\nResources: \\n    * http://www.ilincs.org/ilincs/APIinfo\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here we will be creating an SQL database to store disease signatures from iLINCS!\n",
    "\n",
    "We will filter those signatures which belong to diseases - which are >9,000 signatures\n",
    "from the iLINCS database.\n",
    "\n",
    "Resources: \n",
    "    * http://www.ilincs.org/ilincs/APIinfo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528e5a0-747e-479a-8766-7910db900150",
   "metadata": {},
   "source": [
    "## Create Table Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "101377be-c525-46ee-9e60-bc2f3f80f07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 15:51:58,318 - INFO - Data loaded successfully.\n",
      "/tmp/ipykernel_809387/2114282074.py:45: DtypeWarning: Columns (0,3,7,8,9,10,15,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_data = pd.read_csv(path_data)\n",
      "2023-12-13 15:51:58,782 - INFO - Data loaded successfully.\n",
      "2023-12-13 15:51:58,833 - INFO - Shape of filtered DataFrame: (1087, 5)\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports, Variables, Functions\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# SQL variables\n",
    "dbname = \"ilincs\"\n",
    "user = \"ddalton\"\n",
    "password = \"Teclado$$$111\"\n",
    "host = \"localhost\"\n",
    "path_data = \"../../data/iLINCS/datasets.csv\"\n",
    "path_signature = \"../../data/iLINCS/signatures.csv\"\n",
    "table_name = \"datasets\"\n",
    "primary_key = \"experiment\"\n",
    "int_columns = [\"nsamples\"]  # INT columns - rest TEXT\n",
    "drop_table = True\n",
    "columns_of_interest = [\"geolink\", \"publink\", \"experiment\", \"organism\", \"description\"]\n",
    "ilincs_2_sql_columns = {\n",
    "    \"experiment\": \"dataset_id\",\n",
    "    \"geolink\": \"geo_link\",\n",
    "    \"publink\": \"pub_link\",\n",
    "    \"organism\": \"organism\",\n",
    "    \"description\": \"description\",\n",
    "}\n",
    "\n",
    "\n",
    "# functions\n",
    "def get_disease_datasets():\n",
    "    \"\"\"\n",
    "    Get Disease Datasets\n",
    "    Function to retrieve from those filtered signatures the datasetid\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    Return:\n",
    "    datasetid: list()\n",
    "        List of unique dataset id's\n",
    "    \"\"\"\n",
    "    path_data = \"../../data/iLINCS/signatures.csv\"\n",
    "    filter_df = lambda df: df[\"libraryid\"] == \"LIB_1\"\n",
    "\n",
    "    # Load Data\n",
    "    try:\n",
    "        df_data = pd.read_csv(path_data)\n",
    "        logging.info(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(\"Data file not found. Please check the file path.\")\n",
    "        exit()\n",
    "\n",
    "    # filter disease signatures\n",
    "    df_data = df_data[filter_df]\n",
    "\n",
    "    return list(df_data[\"datasetid\"].unique())\n",
    "\n",
    "\n",
    "# 2. Load Data\n",
    "try:\n",
    "    df_data = pd.read_csv(path_data)\n",
    "    logging.info(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Data file not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# get unique disease dataset id's\n",
    "unique_datasetid = get_disease_datasets()\n",
    "\n",
    "# filter disease signatures\n",
    "# dataset id's refer to experiment\n",
    "df_data = df_data[df_data[\"experiment\"].isin(unique_datasetid)]\n",
    "\n",
    "# filter columns of interest\n",
    "df_data = df_data[columns_of_interest]\n",
    "\n",
    "# assert we find all dataset id's\n",
    "assert len(unique_datasetid) == len(\n",
    "    df_data[\"experiment\"].unique()\n",
    "), \"Error, Dataset Table does not conain all unique datasetid\"\n",
    "\n",
    "# get max length for each\n",
    "max_lengths = [\n",
    "    max([len(str(n)) for n in df_data[c].to_list()]) for c in df_data.columns\n",
    "]\n",
    "\n",
    "# Convert specified integer columns and handle NaN by replacing with 0\n",
    "if any(c in df_data.columns for c in int_columns):\n",
    "    for col in list(set(int_columns) & set(df_data.columns)):\n",
    "        df_data[col] = (\n",
    "            pd.to_numeric(df_data[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "        )\n",
    "\n",
    "# For other columns, replace NaN with None (which will become NULL in SQL)\n",
    "for col in df_data.columns:\n",
    "    if col not in int_columns:\n",
    "        df_data[col] = df_data[col].where(pd.notnull(df_data[col]), None)\n",
    "\n",
    "# Drop Duplicate for experiment column\n",
    "df_data = df_data.drop_duplicates(subset=\"experiment\", keep=\"first\")\n",
    "\n",
    "logging.info(f\"Shape of filtered DataFrame: {df_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64dc9597-1a94-4dd9-ba4c-752ce353e5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for c in df_data.columns:\\n    print(f\"######{c}######\\n{df_data[c].value_counts()}\")'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for c in df_data.columns:\n",
    "    print(f\"######{c}######\\n{df_data[c].value_counts()}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "799fc809-f316-4be7-b0aa-ab6cf204b6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 16:15:28,206 - INFO - Connected to the database successfully.\n",
      "2023-12-13 16:15:28,215 - INFO - Table datasets created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table datasets dropped successfully if it existed.\n"
     ]
    }
   ],
   "source": [
    "# 3. Connect with Database\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "    logging.info(\"Connected to the database successfully.\")\n",
    "except psycopg2.OperationalError as e:\n",
    "    logging.error(f\"Unable to connect to the database: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 4. Create Cursor Object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 5. Check if Table Exists and Delete Data if It Does\n",
    "# Check if the table exists and drop it if it does\n",
    "if drop_table:\n",
    "    try:\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n",
    "        conn.commit()\n",
    "        print(f\"Table {table_name} dropped successfully if it existed.\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "cursor.execute(\n",
    "    \"SELECT EXISTS(SELECT * FROM information_schema.tables WHERE table_name=%s)\",\n",
    "    (table_name,),\n",
    ")\n",
    "table_exists = cursor.fetchone()[0]\n",
    "\n",
    "if table_exists:\n",
    "    try:\n",
    "        cursor.execute(f\"DELETE FROM {table_name};\")\n",
    "        conn.commit()\n",
    "        logging.info(f\"Existing data in table {table_name} deleted successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while deleting data from the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()\n",
    "else:\n",
    "    # Create table if it does not exist\n",
    "    column_text = \", \".join(\n",
    "        f\"{ilincs_2_sql_columns.get(c)} VARCHAR({n + 10})\"\n",
    "        if c not in int_columns\n",
    "        else f\"{ilincs_2_sql_columns.get(c)} INT\"\n",
    "        for c, n in zip(df_data.columns, max_lengths)\n",
    "    )\n",
    "    create_table_query = f\"CREATE TABLE {table_name} ({column_text}, PRIMARY KEY({ilincs_2_sql_columns.get(primary_key)}));\"\n",
    "    try:\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        logging.info(f\"Table {table_name} created successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while creating the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "005fc4a9-b177-484a-9372-1b024c855443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 16:19:14,230 - INFO - Data dumped into datasets successfully.\n"
     ]
    }
   ],
   "source": [
    "# 6. Dump Data into Table\n",
    "data_tuples = list(df_data.itertuples(index=False, name=None))\n",
    "insert_query = (\n",
    "    f\"INSERT INTO {table_name} ({', '.join([ilincs_2_sql_columns.get(c) for c in df_data.columns])}) VALUES (%s\"\n",
    "    + \", %s\" * (len(df_data.columns) - 1)\n",
    "    + \")\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    with conn:\n",
    "        with conn.cursor() as curs:\n",
    "            for record in data_tuples:\n",
    "                try:\n",
    "                    curs.execute(insert_query, record)\n",
    "                except psycopg2.Error as e:\n",
    "                    logging.error(f\"Error inserting record {record}: {e}\")\n",
    "                    # Optionally, you can break the loop after logging the first error\n",
    "                    break\n",
    "    logging.info(f\"Data dumped into {table_name} successfully.\")\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"An error occurred while inserting data into the table: {e}\")\n",
    "    conn.rollback()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be09a16-2962-49b1-b4ad-f1c23a35051a",
   "metadata": {},
   "source": [
    "## Create Signature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419f9557-a0b3-439e-b486-fab1277c80d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_810975/3235342070.py:37: DtypeWarning: Columns (0,3,7,8,9,10,15,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_data = pd.read_csv(path_data)\n",
      "2023-12-13 16:38:04,516 - INFO - Data loaded successfully.\n",
      "2023-12-13 16:38:04,557 - INFO - Shape of filtered DataFrame: (9097, 6)\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports, Variables, Functions\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# SQL variables\n",
    "dbname = \"ilincs\"\n",
    "user = \"ddalton\"\n",
    "password = \"Teclado$$$111\"\n",
    "host = \"localhost\"\n",
    "path_data = \"../../data/iLINCS/signatures.csv\"\n",
    "table_name = \"signatures\"\n",
    "primary_key = \"signatureid\"\n",
    "int_columns = [\"nCtrSamples\", \"nTrtSamples\", \"pubChemID\"]  # INT columns - rest TEXT\n",
    "drop_table = True\n",
    "filter_df = lambda df: df[\"libraryid\"] == \"LIB_1\"\n",
    "reference_table = \"datasets\"\n",
    "reference_key = \"dataset_id\"\n",
    "foreign_key = \"dataset_id\"\n",
    "\n",
    "columns_of_interest = [\n",
    "    \"signatureid\",\n",
    "    \"datasetid\",\n",
    "    \"level1\",\n",
    "    \"level2\",\n",
    "    \"tissue\",\n",
    "    \"cellline\",\n",
    "]\n",
    "\n",
    "ilincs_2_sql_columns = {\n",
    "    \"signatureid\": \"signature_id\",\n",
    "    \"datasetid\": \"dataset_id\",\n",
    "    \"level1\": \"condition_1\",\n",
    "    \"level2\": \"condition_2\",\n",
    "    \"tissue\": \"tissue\",\n",
    "    \"cellline\": \"cell_line\",\n",
    "}\n",
    "\n",
    "# 2. Load Data\n",
    "try:\n",
    "    df_data = pd.read_csv(path_data)\n",
    "    logging.info(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Data file not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# filter disease signatures\n",
    "# which are libraryid LIB_1\n",
    "df_data = df_data[filter_df]\n",
    "\n",
    "# filter columns of interest\n",
    "df_data = df_data[columns_of_interest]\n",
    "\n",
    "# get max length for each\n",
    "max_lengths = [\n",
    "    max([len(str(n)) for n in df_data[c].to_list()]) for c in df_data.columns\n",
    "]\n",
    "\n",
    "# Convert specified integer columns and handle NaN by replacing with 0\n",
    "if any(c in df_data.columns for c in int_columns):\n",
    "    for col in list(set(int_columns) & set(df_data.columns)):\n",
    "        df_data[col] = (\n",
    "            pd.to_numeric(df_data[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "        )\n",
    "\n",
    "# For other columns, replace NaN with None (which will become NULL in SQL)\n",
    "for col in df_data.columns:\n",
    "    if col not in int_columns:\n",
    "        df_data[col] = df_data[col].where(pd.notnull(df_data[col]), None)\n",
    "\n",
    "logging.info(f\"Shape of filtered DataFrame: {df_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec46640c-e9f5-4597-874e-794e1a30bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 16:38:56,268 - INFO - Connected to the database successfully.\n",
      "2023-12-13 16:38:56,275 - INFO - Table signatures created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table signatures dropped successfully if it existed.\n"
     ]
    }
   ],
   "source": [
    "# 3. Connect with Database\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "    logging.info(\"Connected to the database successfully.\")\n",
    "except psycopg2.OperationalError as e:\n",
    "    logging.error(f\"Unable to connect to the database: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 4. Create Cursor Object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 5. Check if Table Exists and Delete Data if It Does\n",
    "\n",
    "# Check if the table exists and drop it if it does\n",
    "if drop_table:\n",
    "    try:\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n",
    "        conn.commit()\n",
    "        print(f\"Table {table_name} dropped successfully if it existed.\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "cursor.execute(\n",
    "    \"SELECT EXISTS(SELECT * FROM information_schema.tables WHERE table_name=%s)\",\n",
    "    (table_name,),\n",
    ")\n",
    "table_exists = cursor.fetchone()[0]\n",
    "\n",
    "if table_exists:\n",
    "    try:\n",
    "        cursor.execute(f\"DELETE FROM {table_name};\")\n",
    "        conn.commit()\n",
    "        logging.info(f\"Existing data in table {table_name} deleted successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while deleting data from the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()\n",
    "else:\n",
    "    # Create table if it does not exist\n",
    "    column_text = \", \".join(\n",
    "        f\"{ilincs_2_sql_columns.get(c)} VARCHAR({n + 10})\"\n",
    "        if c not in int_columns\n",
    "        else f\"{ilincs_2_sql_columns.get(c)} INT\"\n",
    "        for c, n in zip(df_data.columns, max_lengths)\n",
    "    )\n",
    "    create_table_query = f\"CREATE TABLE {table_name} ({column_text}, PRIMARY KEY({ilincs_2_sql_columns.get(primary_key)}),FOREIGN KEY ({foreign_key}) REFERENCES {reference_table}({reference_key}));\"\n",
    "    try:\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        logging.info(f\"Table {table_name} created successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while creating the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d8832a-455a-4a3a-9e0b-6378a73a6bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 16:39:36,545 - INFO - Data dumped into signatures successfully.\n"
     ]
    }
   ],
   "source": [
    "# 6. Dump Data into Table\n",
    "data_tuples = list(df_data.itertuples(index=False, name=None))\n",
    "insert_query = (\n",
    "    f\"INSERT INTO {table_name} ({', '.join([ilincs_2_sql_columns.get(c) for c in df_data.columns])}) VALUES (%s\"\n",
    "    + \", %s\" * (len(df_data.columns) - 1)\n",
    "    + \")\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    with conn:\n",
    "        with conn.cursor() as curs:\n",
    "            for record in data_tuples:\n",
    "                try:\n",
    "                    curs.execute(insert_query, record)\n",
    "                except psycopg2.Error as e:\n",
    "                    logging.error(f\"Error inserting record {record}: {e}\")\n",
    "                    # Optionally, you can break the loop after logging the first error\n",
    "                    break\n",
    "    logging.info(f\"Data dumped into {table_name} successfully.\")\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"An error occurred while inserting data into the table: {e}\")\n",
    "    conn.rollback()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f809d-e8dd-4504-a659-eeb6071e8137",
   "metadata": {},
   "source": [
    "## Create Table Signature Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f5675a-0af8-426c-ae0c-945f3a392b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports, Variables, Functions\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import logging, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# SQL variables\n",
    "dbname = \"ilincs\"\n",
    "user = \"ddalton\"\n",
    "password = \"Teclado$$$111\"\n",
    "host = \"localhost\"\n",
    "path_data = \"../../data/iLINCS/signature_vectors\"\n",
    "table_name = \"signature_values\"\n",
    "primary_key = \"signature_id_gene_id\"\n",
    "int_columns = [\"gene_id\"]\n",
    "float_columns = [\"log_diff_exp\", \"p_value\"]\n",
    "drop_table = True\n",
    "foreign_key = \"signature_id\"\n",
    "reference_table = \"signatures\"\n",
    "reference_key = \"signature_id\"\n",
    "\n",
    "columns_of_interest = [\n",
    "    \"signatureID\",\n",
    "    \"ID_geneid\",\n",
    "    \"Name_GeneSymbol\",\n",
    "    \"Value_LogDiffExp\",\n",
    "    \"Significance_pvalue\",\n",
    "    \"signature_id_gene_id\",\n",
    "]\n",
    "\n",
    "ilincs_2_sql_columns = {\n",
    "    \"signatureID\": \"signature_id\",\n",
    "    \"ID_geneid\": \"gene_id\",\n",
    "    \"Name_GeneSymbol\": \"gene_name\",\n",
    "    \"Value_LogDiffExp\": \"log_diff_exp\",\n",
    "    \"Significance_pvalue\": \"p_value\",\n",
    "    \"signature_id_gene_id\": \"signature_id_gene_id\",\n",
    "}\n",
    "\n",
    "\n",
    "# functions\n",
    "def get_disease_signatureids():\n",
    "    \"\"\"\n",
    "    Get Disease Datasets\n",
    "    Function to retrieve from those filtered signatures the datasetid\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    Return:\n",
    "    datasetid: list()\n",
    "        List of unique dataset id's\n",
    "    \"\"\"\n",
    "    path_data = \"../../data/iLINCS/signatures.csv\"\n",
    "    filter_df = lambda df: df[\"libraryid\"] == \"LIB_1\"\n",
    "\n",
    "    # Load Data\n",
    "    try:\n",
    "        df_data = pd.read_csv(path_data)\n",
    "        logging.info(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(\"Data file not found. Please check the file path.\")\n",
    "        exit()\n",
    "\n",
    "    # filter disease signatures\n",
    "    df_data = df_data[filter_df]\n",
    "\n",
    "    return list(df_data[\"signatureid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "725b270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data\n",
    "\n",
    "if os.path.exists(\"../../data/iLINCS/disease_signature_vectors.csv\"):\n",
    "    df_data = pd.read_csv(\"../../data/iLINCS/disease_signature_vectors.csv\")\n",
    "else:\n",
    "    # get signature ids\n",
    "    signature_ids = get_disease_signatureids()\n",
    "    loop = 0\n",
    "    # get data\n",
    "    for signature_id in tqdm(signature_ids):\n",
    "        if loop == 0:\n",
    "            try:\n",
    "                df_data = pd.read_csv(os.path.join(path_data, signature_id + \".csv\"))\n",
    "                loop = 1\n",
    "            except FileNotFoundError:\n",
    "                logging.error(\"Data file not found. Please check the file path.\")\n",
    "                exit()\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(path_data, signature_id + \".csv\"))\n",
    "                df_data = pd.concat([df, df_data])\n",
    "            except FileNotFoundError:\n",
    "                logging.error(\"Data file not found. Please check the file path.\")\n",
    "                exit()\n",
    "\n",
    "    # save dataframe\n",
    "    try:\n",
    "        df_data.to_csv(\"../../data/iLINCS/disease_signature_vectors.csv\", index=False)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data to csv: {e}\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd0dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create primary key column\n",
    "df_data[\"signature_id_gene_id\"] = (\n",
    "    df_data[\"signatureID\"] + \"_\" + df_data[\"ID_geneid\"].astype(str)\n",
    ")\n",
    "\n",
    "# filter columns of interest\n",
    "df_data = df_data[columns_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad2f064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 14:37:20,116 - INFO - Shape of filtered DataFrame: (144545011, 6)\n",
      "2023-12-18 14:37:20,116 - INFO - Columnns of filtered DataFrame: Index(['signatureID', 'ID_geneid', 'Name_GeneSymbol', 'Value_LogDiffExp',\n",
      "       'Significance_pvalue', 'signature_id_gene_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Shape of filtered DataFrame: {df_data.shape}\")\n",
    "logging.info(f\"Columnns of filtered DataFrame: {df_data.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a77cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max length for each\n",
    "d_max_lengths = {\n",
    "    c: max([len(str(n)) for n in df_data[c].to_list()]) for c in df_data.columns\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41357150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 14:39:09,901 - INFO - Max Lengths: {'signatureID': 8, 'ID_geneid': 9, 'Name_GeneSymbol': 22, 'Value_LogDiffExp': 23, 'Significance_pvalue': 23, 'signature_id_gene_id': 18}\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Max Lengths: {d_max_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "118d871c-64f6-43da-bbb1-f71a8fe8b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 14:39:09,921 - INFO - Connected to the database successfully.\n",
      "2023-12-18 14:39:09,936 - INFO - Table signature_values dropped successfully if it existed.\n",
      "2023-12-18 14:39:09,951 - INFO - Table signature_values created successfully.\n"
     ]
    }
   ],
   "source": [
    "# 3. Connect with Database\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "    logging.info(\"Connected to the database successfully.\")\n",
    "except psycopg2.OperationalError as e:\n",
    "    logging.error(f\"Unable to connect to the database: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 4. Create Cursor Object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 5. Check if Table Exists and Delete Data if It Does\n",
    "\n",
    "# Check if the table exists and drop it if it does\n",
    "if drop_table:\n",
    "    try:\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n",
    "        conn.commit()\n",
    "        logging.info(f\"Table {table_name} dropped successfully if it existed.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.info(f\"An error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "cursor.execute(\n",
    "    \"SELECT EXISTS(SELECT * FROM information_schema.tables WHERE table_name=%s)\",\n",
    "    (table_name,),\n",
    ")\n",
    "table_exists = cursor.fetchone()[0]\n",
    "\n",
    "if table_exists:\n",
    "    try:\n",
    "        cursor.execute(f\"DELETE FROM {table_name};\")\n",
    "        conn.commit()\n",
    "        logging.info(f\"Existing data in table {table_name} deleted successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while deleting data from the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()\n",
    "else:\n",
    "    # Create table if it does not exist\n",
    "    column_text_list = list()\n",
    "    for c in df_data.columns:\n",
    "        if ilincs_2_sql_columns.get(c) in int_columns:\n",
    "            column_text_list.append(f\"{ilincs_2_sql_columns.get(c)} INT\")\n",
    "        elif ilincs_2_sql_columns.get(c) in float_columns:\n",
    "            column_text_list.append(f\"{ilincs_2_sql_columns.get(c)} FLOAT\")\n",
    "        else:\n",
    "            column_text_list.append(\n",
    "                f\"{ilincs_2_sql_columns.get(c)} VARCHAR({d_max_lengths[c] + 10})\"\n",
    "            )\n",
    "\n",
    "    column_text = \", \".join(column_text_list)\n",
    "\n",
    "    create_table_query = f\"CREATE TABLE {table_name} ({column_text}, PRIMARY KEY({ilincs_2_sql_columns.get(primary_key)}),FOREIGN KEY ({foreign_key}) REFERENCES {reference_table}({reference_key}));\"\n",
    "    try:\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        logging.info(f\"Table {table_name} created successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while creating the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a51448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 14:39:09,955 - INFO - Table query created with:\n",
      "CREATE TABLE signature_values (signature_id VARCHAR(18), gene_id INT, gene_name VARCHAR(32), log_diff_exp FLOAT, p_value FLOAT, signature_id_gene_id VARCHAR(28), PRIMARY KEY(signature_id_gene_id),FOREIGN KEY (signature_id) REFERENCES signatures(signature_id));\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Table query created with:\\n{create_table_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "136708c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 16:43:32,793 - INFO - Data dumped into signature_values successfully.\n"
     ]
    }
   ],
   "source": [
    "# 6. Dump Data into Table\n",
    "data_tuples = list(df_data.itertuples(index=False, name=None))\n",
    "insert_query = (\n",
    "    f\"INSERT INTO {table_name} ({', '.join([ilincs_2_sql_columns.get(c) for c in df_data.columns])}) VALUES (%s\"\n",
    "    + \", %s\" * (len(df_data.columns) - 1)\n",
    "    + \")\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    with conn:\n",
    "        with conn.cursor() as curs:\n",
    "            for record in data_tuples:\n",
    "                try:\n",
    "                    curs.execute(insert_query, record)\n",
    "                except psycopg2.Error as e:\n",
    "                    logging.error(f\"Error inserting record {record}: {e}\")\n",
    "                    # Optionally, you can break the loop after logging the first error\n",
    "                    break\n",
    "    logging.info(f\"Data dumped into {table_name} successfully.\")\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"An error occurred while inserting data into the table: {e}\")\n",
    "    conn.rollback()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea198aec",
   "metadata": {},
   "source": [
    "## Create Table MeSH Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14bf7c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 18:53:31,930 - INFO - Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-12-19 18:53:31,930 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports, Variables, Functions\n",
    "# imports\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time, re\n",
    "from Bio import Entrez\n",
    "import logging\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "# variables\n",
    "base_url = \"http://www.ilincs.org/api\"\n",
    "\n",
    "\n",
    "# functions\n",
    "def extract_pmid_from_publink(publink):\n",
    "    \"\"\"Extract the PubMed ID from the provided publink.\"\"\"\n",
    "    pmid_match = re.search(r\"term=(\\d+)\\[UID\\]\", publink)\n",
    "    if pmid_match:\n",
    "        return pmid_match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_disease_datasets():\n",
    "    \"\"\"\n",
    "    Get Disease Datasets\n",
    "    Function to retrieve from those filtered signatures the datasetid\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    Return:\n",
    "    datasetid: list()\n",
    "        List of unique dataset id's\n",
    "    \"\"\"\n",
    "    path_data = \"../../data/iLINCS/signatures.csv\"\n",
    "    filter_df = lambda df: df[\"libraryid\"] == \"LIB_1\"\n",
    "\n",
    "    # Load Data\n",
    "    try:\n",
    "        df_data = pd.read_csv(path_data)\n",
    "        logging.info(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(\"Data file not found. Please check the file path.\")\n",
    "        exit()\n",
    "\n",
    "    # filter disease signatures\n",
    "    df_data = df_data[filter_df]\n",
    "\n",
    "    return list(df_data[\"datasetid\"].unique())\n",
    "\n",
    "\n",
    "def fetch_dataset_metadata(dataset_id):\n",
    "    \"\"\"Fetch dataset metadata/description for a given dataset.\"\"\"\n",
    "    endpoint = f\"{base_url}/PublicDatasets/{dataset_id}\"\n",
    "    response = requests.get(endpoint)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # assuming the response is in JSON format\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_pmid_from_geo_via_eutils(geo_id):\n",
    "    # Use elink to establish links between GEO and PubMed databases\n",
    "    handle = Entrez.elink(dbfrom=\"gds\", db=\"pubmed\", id=geo_id[3:])\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    # Extract the PMID from the linked records\n",
    "    # if it has LinkSetDb report else return None\n",
    "    if len(record[0][\"LinkSetDb\"]) > 0:\n",
    "        return record[0][\"LinkSetDb\"][0][\"Link\"][0][\"Id\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_mesh_terms_from_pubmed(pmid, max_retries=10, retry_delay=5):\n",
    "    \"\"\"Fetch MeSH terms for a given PubMed ID, with retries.\"\"\"\n",
    "    if not pmid:\n",
    "        return []\n",
    "\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    params = {\"db\": \"pubmed\", \"id\": pmid, \"retmode\": \"xml\"}\n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < max_retries:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the XML response to extract MeSH terms and tree numbers\n",
    "            root = ET.fromstring(response.text)\n",
    "            mesh_terms = [\n",
    "                descriptor.findtext(\"DescriptorName\")\n",
    "                for descriptor in root.findall(\".//MeshHeading\")\n",
    "            ]\n",
    "\n",
    "            mesh_tree_numbers = []\n",
    "            for descriptor in root.findall(\".//MeshHeading\"):\n",
    "                descriptor_ui = descriptor.find(\"DescriptorName\").get(\"UI\")\n",
    "                tree_numbers = root.findall(\n",
    "                    f\".//DescriptorRecord[DescriptorUI='{descriptor_ui}']/TreeNumberList/TreeNumber\"\n",
    "                )\n",
    "                mesh_tree_numbers.extend(\n",
    "                    [tree_number.text for tree_number in tree_numbers]\n",
    "                )\n",
    "\n",
    "            return mesh_terms, mesh_tree_numbers\n",
    "\n",
    "        else:\n",
    "            print(\n",
    "                f\"Attempt {attempts + 1} failed: Error fetching MeSH terms for PMID {pmid}: {response.text}\"\n",
    "            )\n",
    "            time.sleep(retry_delay)\n",
    "            attempts += 1\n",
    "\n",
    "    print(\"Max retries reached. Failed to fetch MeSH terms.\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3401451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15775/562292779.py:43: DtypeWarning: Columns (0,3,7,8,9,10,15,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_data = pd.read_csv(path_data)\n"
     ]
    }
   ],
   "source": [
    "datasets = get_disease_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3fe2196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 18:53:43,637 - INFO - Nº of datasets: 117\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Nº of datasets: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "648a932b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1087 [00:00<?, ?it/s]/home/ddalton/miniconda3/envs/disease_sig/lib/python3.10/site-packages/Bio/Entrez/__init__.py:658: UserWarning: \n",
      "Email address is not specified.\n",
      "\n",
      "To make use of NCBI's E-utilities, NCBI requires you to specify your\n",
      "email address with each request.  As an example, if your email address\n",
      "is A.N.Other@example.com, you can specify it as follows:\n",
      "   from Bio import Entrez\n",
      "   Entrez.email = 'A.N.Other@example.com'\n",
      "In case of excessive usage of the E-utilities, NCBI will attempt to contact\n",
      "a user at the email address provided before blocking access to the\n",
      "E-utilities.\n",
      "  warnings.warn(\n",
      "  1%|          | 10/1087 [00:09<16:27,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 20197764: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 26/1087 [00:30<16:05,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 20105310: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 62/1087 [01:13<16:20,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 21029402: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 89/1087 [01:47<16:07,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 21266183: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 101/1087 [02:04<15:31,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 21705112: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 108/1087 [02:17<19:47,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 22108827: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 124/1087 [02:38<15:15,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 21862633: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 125/1087 [02:44<41:23,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 22034635: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 142/1087 [03:07<16:44,  1.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 21625507: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 168/1087 [03:40<16:52,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 20678967: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 177/1087 [03:55<16:15,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 22073175: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 184/1087 [04:07<16:50,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 22021740: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 185/1087 [04:13<40:24,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 21408152: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 200/1087 [04:33<13:40,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 21346816: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 208/1087 [04:46<16:09,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 17595242: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 276/1087 [05:57<11:41,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 17390049: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 337/1087 [07:04<11:16,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 16958858: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 367/1087 [07:37<10:37,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 16682498: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 383/1087 [07:58<10:28,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 16795038: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 389/1087 [08:09<13:36,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 15548687: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 408/1087 [08:34<10:50,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 15958562: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 454/1087 [09:23<09:27,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 15558013: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 474/1087 [09:46<09:29,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 15592430: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 517/1087 [10:38<09:31,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 15579294: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 529/1087 [10:54<08:47,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 15985639: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 549/1087 [11:20<08:35,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 15033914: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 550/1087 [11:26<23:03,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: Error fetching MeSH terms for PMID 14973112: {\"error\":\"API rate limit exceeded\",\"api-key\":\"84.88.74.211\",\"count\":\"4\",\"limit\":\"3\"}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1087/1087 [20:21<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# define dictionary\n",
    "d_dataset_2_mesh = dict()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "i = 0\n",
    "for dataset_id in tqdm(datasets):\n",
    "    # retrieve metadata from iLINCS for specific datasetid\n",
    "    metadata = fetch_dataset_metadata(dataset_id)\n",
    "\n",
    "    # retrieve from metadata pmid\n",
    "    pmid = extract_pmid_from_publink(metadata[\"publink\"])\n",
    "\n",
    "    if pmid:\n",
    "        # if pmid listed retrieve from pmid associated MeSH terms\n",
    "        mesh_terms = fetch_mesh_terms_from_pubmed(pmid)\n",
    "\n",
    "        # print(f\"Found MeSH terms for dataset {dataset_id}: {mesh_terms}\")\n",
    "        d_dataset_2_mesh[dataset_id] = mesh_terms\n",
    "\n",
    "    else:\n",
    "        # if pmid NOT listed try and retrieve it by accessing GEO website &\n",
    "        # retrieving by web scrapping the pmid\n",
    "        pmid = get_pmid_from_geo_via_eutils(metadata[\"SourceID\"])\n",
    "\n",
    "        if pmid:\n",
    "            # if pmid listed retrieve pmid associated MeSH terms\n",
    "            mesh_terms = fetch_mesh_terms_from_pubmed(pmid)\n",
    "            # print(f\"Found MeSH terms for dataset {dataset_id}: {mesh_terms}\")\n",
    "            d_dataset_2_mesh[dataset_id] = mesh_terms\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"No valid PMID found for dataset {dataset_id}.\")\n",
    "    if i == 0:\n",
    "        logging.info(f\"Example of metadata: {d_dataset_2_mesh}\")\n",
    "        i += 1\n",
    "end_time = time.time()\n",
    "logging.info(\n",
    "    f\"Finished Retrieving MeSH terms for Datasets. Total time taken: %.4f seconds\"\n",
    "    % (end_time - start_time)\n",
    ")\n",
    "\n",
    "\n",
    "# save dictionary\n",
    "try:\n",
    "    with open(\"../../data/iLINCS/dataset_2_mesh.json\", \"w\") as f:\n",
    "        json.dump(d_dataset_2_mesh, f)\n",
    "        print(\"Saved!\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving data to json: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2056c2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 18:40:58,611 - INFO - Starting to Get All Signatures: \n",
      "2023-12-19 18:41:22,901 - INFO - Finished Getting All Signatures. Total time taken: 24.2891 seconds\n",
      "2023-12-19 18:41:22,967 - INFO - Starting to Retrieve MeSH terms for Datasets: \n",
      "2023-12-19 18:41:31,575 - INFO - Finished Retrieving MeSH terms for Datasets. Total time taken: 8.6083 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"iLINCS\n",
    "\n",
    "The exercise here is to quantify HOW many diseases are there for which we have \"disease\" signatures\n",
    "\n",
    "Structure:\n",
    "    1. Imports, Variables, Functions\n",
    "    2. Retrieve MeSH terms\n",
    "    3. Retrieve Signature Datasets\n",
    "    4. Maps MeSH terms to Signatures\n",
    "    5. Plot Results\n",
    "\"\"\"\n",
    "\n",
    "# 1. Imports, Variables, Functions\n",
    "# imports\n",
    "import requests, json, re\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from Bio import Entrez\n",
    "import logging\n",
    "import time\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "\n",
    "# variables\n",
    "Entrez.email = \"dylandaltonsub@gmail.com\"\n",
    "base_url = \"http://www.ilincs.org/api\"\n",
    "doi_data_path = \"../../data/DiseaseOntology/doid.obo\"\n",
    "mesh_file_path = \"../../data/MeSH/desc2023.xml\"\n",
    "d_dataset_2_mesh = dict()\n",
    "d_signature_2_mesh = dict()\n",
    "d_mesh_symbol_2_term = dict()\n",
    "filter_criteria = lambda s: (s[\"factor\"] == \"disease.state\") and (\n",
    "    \"normal\" in s[\"level2\"] or \"control\" in s[\"level2\"] or \"healthy\" in s[\"level2\"]\n",
    ")\n",
    "\n",
    "\n",
    "# functions\n",
    "def fetch_disease_signatures(factor):\n",
    "    \"\"\"Fetch Disease Signatures\"\"\"\n",
    "\n",
    "    # Construct the filtering JSON based on provided example\n",
    "    # filter_json = {\n",
    "    #     \"where\": {\n",
    "    #         \"factor\": factor,\n",
    "    #         #\"baseline\": baseline\n",
    "    #     }\n",
    "    # }\n",
    "    # filter_str = json.dumps(filter_json)\n",
    "\n",
    "    endpoint = f\"{base_url}/SignatureMeta\"\n",
    "    # response = requests.get(endpoint, params={\"filter\": filter_str})\n",
    "    response = requests.get(endpoint)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # assuming the response is in JSON format\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "        return []\n",
    "\n",
    "\n",
    "def extract_disease_names_from_obo(file_path):\n",
    "    \"\"\"\n",
    "    Extracts disease names from an OBO formatted file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the OBO file.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of disease names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open and read the content of the OBO file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # The OBO format divides entries using '[Term]'. We split the content based on this to get individual entries.\n",
    "    terms = content.split(\"[Term]\")\n",
    "\n",
    "    disease_names = []  # List to store extracted disease names\n",
    "\n",
    "    # Iterate over each term/entry\n",
    "    for term in terms:\n",
    "        # Use a regular expression to search for the line that starts with 'name: '\n",
    "        # This line contains the name of the disease.\n",
    "        match = re.search(r\"name: (.+)\", term)\n",
    "\n",
    "        # If a match is found (i.e., the term has a name), extract it and add to the list\n",
    "        if match:\n",
    "            disease_name = match.group(\n",
    "                1\n",
    "            )  # The actual name is captured in the first group of the regex\n",
    "            disease_names.append(disease_name)\n",
    "\n",
    "    return disease_names\n",
    "\n",
    "\n",
    "def parse_mesh_data(file_path):\n",
    "    \"\"\"Parse MeSH XML data and extract disease terms.\"\"\"\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract disease terms\n",
    "    disease_terms = []\n",
    "    for descriptor in root.findall(\"DescriptorRecord\"):\n",
    "        term = descriptor.find(\"DescriptorName/String\").text\n",
    "        disease_terms.append(term)\n",
    "\n",
    "    return disease_terms, None\n",
    "\n",
    "\n",
    "def parse_mesh_data(file_path):\n",
    "    \"\"\"Parse MeSH XML data and extract disease terms.\n",
    "\n",
    "    Retrieve the Botom-Most disease terms which contain the most specific\n",
    "    information for a disease.\n",
    "\n",
    "    Parameters:\n",
    "        file_path: str()\n",
    "\n",
    "    Return:\n",
    "        disease_terms: list()\n",
    "        list_tree_numbers: list()\"\"\"\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract disease terms\n",
    "    disease_terms = list()\n",
    "    list_tree_numbers = list()\n",
    "    for descriptor in root.findall(\"DescriptorRecord\"):\n",
    "        # Check if the term is under the category of diseases\n",
    "        tree_numbers = descriptor.findall(\"TreeNumberList/TreeNumber\")\n",
    "        for tree_number in tree_numbers:\n",
    "            # This is a basic check for TreeNumbers starting with 'C' which usually denotes diseases in MeSH\n",
    "            # You might need to adjust this based on the specific structure of your XML file\n",
    "            if tree_number.text.startswith(\"C\"):\n",
    "                list_tree_numbers.append(tree_number.text)\n",
    "                term = descriptor.find(\"DescriptorName/String\").text\n",
    "                disease_terms.append(term)\n",
    "                break  # Break after adding the term to avoid duplicates\n",
    "\n",
    "    return disease_terms, list_tree_numbers\n",
    "\n",
    "\n",
    "def extract_pmid_from_publink(publink):\n",
    "    \"\"\"Extract the PubMed ID from the provided publink.\"\"\"\n",
    "    pmid_match = re.search(r\"term=(\\d+)\\[UID\\]\", publink)\n",
    "    if pmid_match:\n",
    "        return pmid_match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_pmid_from_geo_via_eutils(geo_id):\n",
    "    # Use elink to establish links between GEO and PubMed databases\n",
    "    handle = Entrez.elink(dbfrom=\"gds\", db=\"pubmed\", id=geo_id[3:])\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    # Extract the PMID from the linked records\n",
    "    # if it has LinkSetDb report else return None\n",
    "    if len(record[0][\"LinkSetDb\"]) > 0:\n",
    "        return record[0][\"LinkSetDb\"][0][\"Link\"][0][\"Id\"]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_disease_signatures():\n",
    "    \"\"\"Fetch Disease Signatures\"\"\"\n",
    "\n",
    "    # Construct the filtering JSON based on provided example\n",
    "    # filter_json = {\n",
    "    #     \"where\": {\n",
    "    #         \"factor\": factor,\n",
    "    #         #\"baseline\": baseline\n",
    "    #     }\n",
    "    # }\n",
    "    # filter_str = json.dumps(filter_json)\n",
    "\n",
    "    endpoint = f\"{base_url}/SignatureMeta\"\n",
    "    # response = requests.get(endpoint, params={\"filter\": filter_str})\n",
    "    response = requests.get(endpoint)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # assuming the response is in JSON format\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "        return []\n",
    "\n",
    "\n",
    "def fetch_dataset_metadata(dataset_id):\n",
    "    \"\"\"Fetch dataset metadata/description for a given dataset.\"\"\"\n",
    "    endpoint = f\"{base_url}/PublicDatasets/{dataset_id}\"\n",
    "    response = requests.get(endpoint)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()  # assuming the response is in JSON format\n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_mesh_terms_from_pubmed(pmid):\n",
    "    \"\"\"Fetch MeSH terms for a given PubMed ID.\"\"\"\n",
    "    if not pmid:\n",
    "        return []\n",
    "\n",
    "    base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "    params = {\"db\": \"pubmed\", \"id\": pmid, \"retmode\": \"xml\"}\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching MeSH terms for PMID {pmid}: {response.text}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the XML response to extract MeSH terms\n",
    "    root = ET.fromstring(response.text)\n",
    "    mesh_terms = [\n",
    "        descriptor.findtext(\"DescriptorName\")\n",
    "        for descriptor in root.findall(\".//MeshHeading\")\n",
    "    ]\n",
    "\n",
    "    # Parse the XML response to extract MeSH tree numbers\n",
    "    mesh_tree_numbers = list()\n",
    "    for descriptor in root.findall(\".//MeshHeading\"):\n",
    "        # Find the DescriptorName element and get its UI attribute\n",
    "        descriptor_ui = descriptor.find(\"DescriptorName\").get(\"UI\")\n",
    "        # Use the UI to find the corresponding TreeNumberList/TreeNumber elements\n",
    "        tree_numbers = root.findall(\n",
    "            f\".//DescriptorRecord[DescriptorUI='{descriptor_ui}']/TreeNumberList/TreeNumber\"\n",
    "        )\n",
    "        mesh_tree_numbers.extend([tree_number.text for tree_number in tree_numbers])\n",
    "\n",
    "    return mesh_terms, mesh_tree_numbers\n",
    "\n",
    "\n",
    "def extract_pmid_from_publink(publink):\n",
    "    \"\"\"Extract the PubMed ID from the provided publink.\"\"\"\n",
    "    pmid_match = re.search(r\"term=(\\d+)\\[UID\\]\", publink)\n",
    "    if pmid_match:\n",
    "        return pmid_match.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_mesh_term_tree_number_mapping(mesh_xml_file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Build a mapping of MeSH terms to their tree numbers from the MeSH XML file.\n",
    "\n",
    "    Parameters:\n",
    "    - mesh_xml_file_path (str): The file path to the MeSH XML file.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are MeSH terms and values are lists of associated tree numbers.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(mesh_xml_file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    mesh_term_2_symbol = dict()\n",
    "    mesh_symbol_2_term = dict()\n",
    "    for descriptor in root.findall(\"DescriptorRecord\"):\n",
    "        term = descriptor.find(\"DescriptorName/String\").text\n",
    "        tree_numbers = [\n",
    "            tree_number.text\n",
    "            for tree_number in descriptor.findall(\"TreeNumberList/TreeNumber\")\n",
    "        ]\n",
    "        for tree_number in tree_numbers:\n",
    "            mesh_symbol_2_term[tree_number] = term\n",
    "        mesh_term_2_symbol[term] = tree_numbers\n",
    "\n",
    "    return mesh_term_2_symbol, mesh_symbol_2_term\n",
    "\n",
    "\n",
    "# 2. Retrieve MeSH terms\n",
    "# retrieve disease terms and store in dictionary\n",
    "disease_names_mesh, symbol_mesh = parse_mesh_data(file_path=mesh_file_path)\n",
    "d_mesh_symbol_2_term = dict(zip(symbol_mesh, disease_names_mesh))\n",
    "\n",
    "# 3. Retrieve Signature Datasets\n",
    "start_time = time.time()\n",
    "logging.info(\"Starting to Get All Signatures: \")\n",
    "\n",
    "# get all signatures\n",
    "signatures = fetch_disease_signatures()\n",
    "\n",
    "end_time = time.time()\n",
    "logging.info(\n",
    "    f\"Finished Getting All Signatures. Total time taken: %.4f seconds\"\n",
    "    % (end_time - start_time)\n",
    ")\n",
    "\n",
    "# filter signatures to only those which have as level2 \"normal\" & factor \"disease state\"\n",
    "# get unique datasets for this filtering\n",
    "datasets = list(set([s[\"datasetid\"] for s in signatures if filter_criteria(s)]))\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "logging.info(\"Starting to Retrieve MeSH terms for Datasets: \")\n",
    "\n",
    "for dataset_id in datasets[:10]:\n",
    "    # retrieve metadata from iLINCS for specific datasetid\n",
    "    metadata = fetch_dataset_metadata(dataset_id)\n",
    "\n",
    "    # retrieve from metadata pmid\n",
    "    pmid = extract_pmid_from_publink(metadata[\"publink\"])\n",
    "\n",
    "    if pmid:\n",
    "        # if pmid listed retrieve from pmid associated MeSH terms\n",
    "        mesh_terms = fetch_mesh_terms_from_pubmed(pmid)\n",
    "\n",
    "        # print(f\"Found MeSH terms for dataset {dataset_id}: {mesh_terms}\")\n",
    "        d_dataset_2_mesh[dataset_id] = mesh_terms\n",
    "\n",
    "    else:\n",
    "        # if pmid NOT listed try and retrieve it by accessing GEO website &\n",
    "        # retrieving by web scrapping the pmid\n",
    "        pmid = get_pmid_from_geo_via_eutils(metadata[\"SourceID\"])\n",
    "\n",
    "        if pmid:\n",
    "            # if pmid listed retrieve pmid associated MeSH terms\n",
    "            mesh_terms = fetch_mesh_terms_from_pubmed(pmid)\n",
    "            # print(f\"Found MeSH terms for dataset {dataset_id}: {mesh_terms}\")\n",
    "            d_dataset_2_mesh[dataset_id] = mesh_terms\n",
    "        else:\n",
    "            pass\n",
    "            # print(f\"No valid PMID found for dataset {dataset_id}.\")\n",
    "\n",
    "end_time = time.time()\n",
    "logging.info(\n",
    "    f\"Finished Retrieving MeSH terms for Datasets. Total time taken: %.4f seconds\"\n",
    "    % (end_time - start_time)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90a6e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_term_2_symbol, mesh_symbol_2_term = build_mesh_term_tree_number_mapping(\n",
    "    mesh_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8990b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_translation_all = list()\n",
    "for v, MeSH_terms in d_dataset_2_mesh.items():\n",
    "    tree_symbols = list()\n",
    "    failed_translation = list()\n",
    "    for MeSH_term in MeSH_terms[0]:\n",
    "        if len(mesh_term_2_symbol[MeSH_term]) > 0:\n",
    "            tree_symbols.append(mesh_term_2_symbol[MeSH_term])\n",
    "            # print(mesh_term_2_symbol[MeSH_term])\n",
    "        else:\n",
    "            failed_translation.append(MeSH_term)\n",
    "            failed_translation_all.append(MeSH_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b7ae09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gdsGDS3903': (['Cluster Analysis',\n",
       "   'Databases, Genetic',\n",
       "   'Extracellular Matrix',\n",
       "   'Gene Expression Profiling',\n",
       "   'Humans',\n",
       "   'Intracranial Aneurysm',\n",
       "   'Oligonucleotide Array Sequence Analysis',\n",
       "   'Reverse Transcriptase Polymerase Chain Reaction'],\n",
       "  []),\n",
       " 'gdsGDS3902': (['Chromatin',\n",
       "   'Cluster Analysis',\n",
       "   'Gene Expression Profiling',\n",
       "   'Gene Expression Regulation, Leukemic',\n",
       "   'HeLa Cells',\n",
       "   'Humans',\n",
       "   'Leukemia, Lymphocytic, Chronic, B-Cell',\n",
       "   'MicroRNAs',\n",
       "   'Microarray Analysis',\n",
       "   'Oncogene Proteins v-myb',\n",
       "   'Promoter Regions, Genetic',\n",
       "   'Protein Binding',\n",
       "   'Transcription, Genetic',\n",
       "   'Transfection',\n",
       "   'Tumor Cells, Cultured'],\n",
       "  []),\n",
       " 'gdsGDS2855': (['Biopsy',\n",
       "   'Child',\n",
       "   'DNA Fingerprinting',\n",
       "   'Gene Expression Profiling',\n",
       "   'Humans',\n",
       "   'Lamin Type A',\n",
       "   'Membrane Proteins',\n",
       "   'Models, Statistical',\n",
       "   'Muscle, Skeletal',\n",
       "   'Muscular Dystrophies',\n",
       "   'Muscular Dystrophy, Emery-Dreifuss',\n",
       "   'Mutation',\n",
       "   'MyoD Protein',\n",
       "   'Nuclear Envelope',\n",
       "   'Nuclear Proteins',\n",
       "   'Oligonucleotide Array Sequence Analysis',\n",
       "   'Protein Binding',\n",
       "   'RNA, Messenger',\n",
       "   'Regeneration',\n",
       "   'Reverse Transcriptase Polymerase Chain Reaction',\n",
       "   'Thymopoietins',\n",
       "   'Transcription, Genetic'],\n",
       "  []),\n",
       " 'gdsGDS3057': (['Adult',\n",
       "   'Biomarkers, Tumor',\n",
       "   'Cyclin A',\n",
       "   'Cyclin A1',\n",
       "   'Female',\n",
       "   'Gene Expression Regulation, Leukemic',\n",
       "   'Genes, Neoplasm',\n",
       "   'Genes, Wilms Tumor',\n",
       "   'Genetic Markers',\n",
       "   'Humans',\n",
       "   'Interleukin-3 Receptor alpha Subunit',\n",
       "   'Leukemia, Myeloid, Acute',\n",
       "   'Male',\n",
       "   'Middle Aged',\n",
       "   'Oligonucleotide Array Sequence Analysis',\n",
       "   'Receptors, Interleukin-3',\n",
       "   'Tumor Cells, Cultured'],\n",
       "  []),\n",
       " 'gdsGDS470': (['Cervix Uteri',\n",
       "   'Female',\n",
       "   'Humans',\n",
       "   'Neoplasm Staging',\n",
       "   'Oligonucleotide Array Sequence Analysis',\n",
       "   'Reference Values',\n",
       "   'Treatment Outcome',\n",
       "   'Uterine Cervical Neoplasms'],\n",
       "  []),\n",
       " 'gdsGDS2737': (['Adaptor Proteins, Signal Transducing',\n",
       "   'Adult',\n",
       "   'Cytochrome P-450 Enzyme System',\n",
       "   'Endometriosis',\n",
       "   'Endometrium',\n",
       "   'Female',\n",
       "   'Forkhead Box Protein O1',\n",
       "   'Forkhead Transcription Factors',\n",
       "   'Gene Expression Profiling',\n",
       "   'Genetic Linkage',\n",
       "   'Genetic Predisposition to Disease',\n",
       "   'Humans',\n",
       "   'Leiomyoma',\n",
       "   'Oligonucleotide Array Sequence Analysis',\n",
       "   'Progesterone',\n",
       "   'Retinoic Acid 4-Hydroxylase',\n",
       "   'Reverse Transcriptase Polymerase Chain Reaction',\n",
       "   'Tumor Suppressor Proteins',\n",
       "   'Uterine Neoplasms'],\n",
       "  []),\n",
       " 'gdsGDS3869': (['Adult',\n",
       "   'Aged',\n",
       "   'Aged, 80 and over',\n",
       "   'Case-Control Studies',\n",
       "   'Cell Transformation, Neoplastic',\n",
       "   'Disease Progression',\n",
       "   'Female',\n",
       "   'Gene Expression Profiling',\n",
       "   'Gene Expression Regulation',\n",
       "   'Gene Regulatory Networks',\n",
       "   'Humans',\n",
       "   'Leukemia, Lymphocytic, Chronic, B-Cell',\n",
       "   'Lymphocyte Count',\n",
       "   'Lymphoma, B-Cell',\n",
       "   'Male',\n",
       "   'Microarray Analysis',\n",
       "   'Middle Aged',\n",
       "   'Models, Biological',\n",
       "   'T-Lymphocytes'],\n",
       "  []),\n",
       " 'gdsGDS2643': (['B-Lymphocytes',\n",
       "   'Blood Cells',\n",
       "   'Bone Marrow Cells',\n",
       "   'Clone Cells',\n",
       "   'Cluster Analysis',\n",
       "   'Gene Expression Profiling',\n",
       "   'Humans',\n",
       "   'Leukemia, Lymphocytic, Chronic, B-Cell',\n",
       "   'Multiple Myeloma',\n",
       "   'Neoplasm Proteins',\n",
       "   'Oligonucleotide Array Sequence Analysis',\n",
       "   'Plasma Cells',\n",
       "   'Subtraction Technique',\n",
       "   'Transcription, Genetic',\n",
       "   'Waldenstrom Macroglobulinemia'],\n",
       "  [])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_dataset_2_mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f16b1-2756-4b04-9080-33aa156cac6d",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fafed3de-1966-4f52-aaac-bcc0a09055ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19811/276313882.py:27: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "# SQL variables\n",
    "dbname = \"ilincs\"\n",
    "user = \"ddalton\"\n",
    "password = \"Teclado$$$111\"\n",
    "host = \"localhost\"\n",
    "table_name = \"signature_values\"\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "    print(\"Connected to the database successfully.\")\n",
    "except psycopg2.OperationalError as e:\n",
    "    print(f\"Unable to connect to the database: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Query the table\n",
    "try:\n",
    "    query = f\"\"\"\n",
    "    SELECT column_name, data_type \n",
    "    FROM information_schema.columns \n",
    "    WHERE table_name = '{table_name}'\n",
    "    \"\"\"\n",
    "    query = f\"SELECT * FROM {table_name} LIMIT 10;\"  # Adjust the query as needed\n",
    "    query = f\"SELECT * FROM {table_name} WHERE antibodytarget IS NOT NULL;\"\n",
    "    query = f\"SELECT * FROM {table_name} WHERE experiment IS  NULL;\"\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while querying the table: {e}\")\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "# The DataFrame 'df' now contains the first 10 rows of the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a7c423",
   "metadata": {},
   "source": [
    "# Signature CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2878997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Signature CSV files\n",
    "\n",
    "Structure:\n",
    "    1. Imports, Variables, Functions\n",
    "    2. Load Data\n",
    "    3. Save to CSV\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
