{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726775a5-c4b7-4fb4-a36d-ce151bbca008",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "798f78aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHere we will be creating an SQL database to store disease signatures from iLINCS!\\n\\nWe will filter those signatures which belong to diseases - which are >9,000 signatures\\nfrom the iLINCS database.\\n\\nResources: \\n    * http://www.ilincs.org/ilincs/APIinfo\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here we will be creating an SQL database to store disease signatures from iLINCS!\n",
    "\n",
    "We will filter those signatures which belong to diseases - which are >9,000 signatures\n",
    "from the iLINCS database.\n",
    "\n",
    "Resources: \n",
    "    * http://www.ilincs.org/ilincs/APIinfo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9528e5a0-747e-479a-8766-7910db900150",
   "metadata": {},
   "source": [
    "## Create Table Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "101377be-c525-46ee-9e60-bc2f3f80f07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 15:51:58,318 - INFO - Data loaded successfully.\n",
      "/tmp/ipykernel_809387/2114282074.py:45: DtypeWarning: Columns (0,3,7,8,9,10,15,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_data = pd.read_csv(path_data)\n",
      "2023-12-13 15:51:58,782 - INFO - Data loaded successfully.\n",
      "2023-12-13 15:51:58,833 - INFO - Shape of filtered DataFrame: (1087, 5)\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports, Variables, Functions\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# SQL variables\n",
    "dbname = \"ilincs\"\n",
    "user = \"ddalton\"\n",
    "password = \"Teclado$$$111\"\n",
    "host = \"localhost\"\n",
    "path_data = \"../../data/iLINCS/datasets.csv\"\n",
    "path_signature = \"../../data/iLINCS/signatures.csv\"\n",
    "table_name = \"datasets\"\n",
    "primary_key = \"experiment\"\n",
    "int_columns = [\"nsamples\"]  # INT columns - rest TEXT\n",
    "drop_table = True\n",
    "columns_of_interest = [\"geolink\", \"publink\", \"experiment\", \"organism\", \"description\"]\n",
    "ilincs_2_sql_columns = {\n",
    "    \"experiment\": \"dataset_id\",\n",
    "    \"geolink\": \"geo_link\",\n",
    "    \"publink\": \"pub_link\",\n",
    "    \"organism\": \"organism\",\n",
    "    \"description\": \"description\",\n",
    "}\n",
    "\n",
    "\n",
    "# functions\n",
    "def get_disease_datasets():\n",
    "    \"\"\"\n",
    "    Get Disease Datasets\n",
    "    Function to retrieve from those filtered signatures the datasetid\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    Return:\n",
    "    datasetid: list()\n",
    "        List of unique dataset id's\n",
    "    \"\"\"\n",
    "    path_data = \"../../data/iLINCS/signatures.csv\"\n",
    "    filter_df = lambda df: df[\"libraryid\"] == \"LIB_1\"\n",
    "\n",
    "    # Load Data\n",
    "    try:\n",
    "        df_data = pd.read_csv(path_data)\n",
    "        logging.info(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(\"Data file not found. Please check the file path.\")\n",
    "        exit()\n",
    "\n",
    "    # filter disease signatures\n",
    "    df_data = df_data[filter_df]\n",
    "\n",
    "    return list(df_data[\"datasetid\"].unique())\n",
    "\n",
    "\n",
    "# 2. Load Data\n",
    "try:\n",
    "    df_data = pd.read_csv(path_data)\n",
    "    logging.info(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Data file not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# get unique disease dataset id's\n",
    "unique_datasetid = get_disease_datasets()\n",
    "\n",
    "# filter disease signatures\n",
    "# dataset id's refer to experiment\n",
    "df_data = df_data[df_data[\"experiment\"].isin(unique_datasetid)]\n",
    "\n",
    "# filter columns of interest\n",
    "df_data = df_data[columns_of_interest]\n",
    "\n",
    "# assert we find all dataset id's\n",
    "assert len(unique_datasetid) == len(\n",
    "    df_data[\"experiment\"].unique()\n",
    "), \"Error, Dataset Table does not conain all unique datasetid\"\n",
    "\n",
    "# get max length for each\n",
    "max_lengths = [\n",
    "    max([len(str(n)) for n in df_data[c].to_list()]) for c in df_data.columns\n",
    "]\n",
    "\n",
    "# Convert specified integer columns and handle NaN by replacing with 0\n",
    "if any(c in df_data.columns for c in int_columns):\n",
    "    for col in list(set(int_columns) & set(df_data.columns)):\n",
    "        df_data[col] = (\n",
    "            pd.to_numeric(df_data[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "        )\n",
    "\n",
    "# For other columns, replace NaN with None (which will become NULL in SQL)\n",
    "for col in df_data.columns:\n",
    "    if col not in int_columns:\n",
    "        df_data[col] = df_data[col].where(pd.notnull(df_data[col]), None)\n",
    "\n",
    "# Drop Duplicate for experiment column\n",
    "df_data = df_data.drop_duplicates(subset=\"experiment\", keep=\"first\")\n",
    "\n",
    "logging.info(f\"Shape of filtered DataFrame: {df_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64dc9597-1a94-4dd9-ba4c-752ce353e5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for c in df_data.columns:\\n    print(f\"######{c}######\\n{df_data[c].value_counts()}\")'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"for c in df_data.columns:\n",
    "    print(f\"######{c}######\\n{df_data[c].value_counts()}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "799fc809-f316-4be7-b0aa-ab6cf204b6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 16:15:28,206 - INFO - Connected to the database successfully.\n",
      "2023-12-13 16:15:28,215 - INFO - Table datasets created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table datasets dropped successfully if it existed.\n"
     ]
    }
   ],
   "source": [
    "# 3. Connect with Database\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "    logging.info(\"Connected to the database successfully.\")\n",
    "except psycopg2.OperationalError as e:\n",
    "    logging.error(f\"Unable to connect to the database: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 4. Create Cursor Object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 5. Check if Table Exists and Delete Data if It Does\n",
    "# Check if the table exists and drop it if it does\n",
    "if drop_table:\n",
    "    try:\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n",
    "        conn.commit()\n",
    "        print(f\"Table {table_name} dropped successfully if it existed.\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "cursor.execute(\n",
    "    \"SELECT EXISTS(SELECT * FROM information_schema.tables WHERE table_name=%s)\",\n",
    "    (table_name,),\n",
    ")\n",
    "table_exists = cursor.fetchone()[0]\n",
    "\n",
    "if table_exists:\n",
    "    try:\n",
    "        cursor.execute(f\"DELETE FROM {table_name};\")\n",
    "        conn.commit()\n",
    "        logging.info(f\"Existing data in table {table_name} deleted successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while deleting data from the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()\n",
    "else:\n",
    "    # Create table if it does not exist\n",
    "    column_text = \", \".join(\n",
    "        f\"{ilincs_2_sql_columns.get(c)} VARCHAR({n + 10})\"\n",
    "        if c not in int_columns\n",
    "        else f\"{ilincs_2_sql_columns.get(c)} INT\"\n",
    "        for c, n in zip(df_data.columns, max_lengths)\n",
    "    )\n",
    "    create_table_query = f\"CREATE TABLE {table_name} ({column_text}, PRIMARY KEY({ilincs_2_sql_columns.get(primary_key)}));\"\n",
    "    try:\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        logging.info(f\"Table {table_name} created successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while creating the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "005fc4a9-b177-484a-9372-1b024c855443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 16:19:14,230 - INFO - Data dumped into datasets successfully.\n"
     ]
    }
   ],
   "source": [
    "# 6. Dump Data into Table\n",
    "data_tuples = list(df_data.itertuples(index=False, name=None))\n",
    "insert_query = (\n",
    "    f\"INSERT INTO {table_name} ({', '.join([ilincs_2_sql_columns.get(c) for c in df_data.columns])}) VALUES (%s\"\n",
    "    + \", %s\" * (len(df_data.columns) - 1)\n",
    "    + \")\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    with conn:\n",
    "        with conn.cursor() as curs:\n",
    "            for record in data_tuples:\n",
    "                try:\n",
    "                    curs.execute(insert_query, record)\n",
    "                except psycopg2.Error as e:\n",
    "                    logging.error(f\"Error inserting record {record}: {e}\")\n",
    "                    # Optionally, you can break the loop after logging the first error\n",
    "                    break\n",
    "    logging.info(f\"Data dumped into {table_name} successfully.\")\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"An error occurred while inserting data into the table: {e}\")\n",
    "    conn.rollback()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be09a16-2962-49b1-b4ad-f1c23a35051a",
   "metadata": {},
   "source": [
    "## Create Signature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "419f9557-a0b3-439e-b486-fab1277c80d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_810975/3235342070.py:37: DtypeWarning: Columns (0,3,7,8,9,10,15,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_data = pd.read_csv(path_data)\n",
      "2023-12-13 16:38:04,516 - INFO - Data loaded successfully.\n",
      "2023-12-13 16:38:04,557 - INFO - Shape of filtered DataFrame: (9097, 6)\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports, Variables, Functions\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# SQL variables\n",
    "dbname = \"ilincs\"\n",
    "user = \"ddalton\"\n",
    "password = \"Teclado$$$111\"\n",
    "host = \"localhost\"\n",
    "path_data = \"../../data/iLINCS/signatures.csv\"\n",
    "table_name = \"signatures\"\n",
    "primary_key = \"signatureid\"\n",
    "int_columns = [\"nCtrSamples\", \"nTrtSamples\", \"pubChemID\"]  # INT columns - rest TEXT\n",
    "drop_table = True\n",
    "filter_df = lambda df: df[\"libraryid\"] == \"LIB_1\"\n",
    "reference_table = \"datasets\"\n",
    "reference_key = \"dataset_id\"\n",
    "foreign_key = \"dataset_id\"\n",
    "\n",
    "columns_of_interest = [\n",
    "    \"signatureid\",\n",
    "    \"datasetid\",\n",
    "    \"level1\",\n",
    "    \"level2\",\n",
    "    \"tissue\",\n",
    "    \"cellline\",\n",
    "]\n",
    "\n",
    "ilincs_2_sql_columns = {\n",
    "    \"signatureid\": \"signature_id\",\n",
    "    \"datasetid\": \"dataset_id\",\n",
    "    \"level1\": \"condition_1\",\n",
    "    \"level2\": \"condition_2\",\n",
    "    \"tissue\": \"tissue\",\n",
    "    \"cellline\": \"cell_line\",\n",
    "}\n",
    "\n",
    "# 2. Load Data\n",
    "try:\n",
    "    df_data = pd.read_csv(path_data)\n",
    "    logging.info(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Data file not found. Please check the file path.\")\n",
    "    exit()\n",
    "\n",
    "# filter disease signatures\n",
    "# which are libraryid LIB_1\n",
    "df_data = df_data[filter_df]\n",
    "\n",
    "# filter columns of interest\n",
    "df_data = df_data[columns_of_interest]\n",
    "\n",
    "# get max length for each\n",
    "max_lengths = [\n",
    "    max([len(str(n)) for n in df_data[c].to_list()]) for c in df_data.columns\n",
    "]\n",
    "\n",
    "# Convert specified integer columns and handle NaN by replacing with 0\n",
    "if any(c in df_data.columns for c in int_columns):\n",
    "    for col in list(set(int_columns) & set(df_data.columns)):\n",
    "        df_data[col] = (\n",
    "            pd.to_numeric(df_data[col], errors=\"coerce\").fillna(0).astype(int)\n",
    "        )\n",
    "\n",
    "# For other columns, replace NaN with None (which will become NULL in SQL)\n",
    "for col in df_data.columns:\n",
    "    if col not in int_columns:\n",
    "        df_data[col] = df_data[col].where(pd.notnull(df_data[col]), None)\n",
    "\n",
    "logging.info(f\"Shape of filtered DataFrame: {df_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec46640c-e9f5-4597-874e-794e1a30bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 16:38:56,268 - INFO - Connected to the database successfully.\n",
      "2023-12-13 16:38:56,275 - INFO - Table signatures created successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table signatures dropped successfully if it existed.\n"
     ]
    }
   ],
   "source": [
    "# 3. Connect with Database\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "    logging.info(\"Connected to the database successfully.\")\n",
    "except psycopg2.OperationalError as e:\n",
    "    logging.error(f\"Unable to connect to the database: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 4. Create Cursor Object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 5. Check if Table Exists and Delete Data if It Does\n",
    "\n",
    "# Check if the table exists and drop it if it does\n",
    "if drop_table:\n",
    "    try:\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n",
    "        conn.commit()\n",
    "        print(f\"Table {table_name} dropped successfully if it existed.\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "cursor.execute(\n",
    "    \"SELECT EXISTS(SELECT * FROM information_schema.tables WHERE table_name=%s)\",\n",
    "    (table_name,),\n",
    ")\n",
    "table_exists = cursor.fetchone()[0]\n",
    "\n",
    "if table_exists:\n",
    "    try:\n",
    "        cursor.execute(f\"DELETE FROM {table_name};\")\n",
    "        conn.commit()\n",
    "        logging.info(f\"Existing data in table {table_name} deleted successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while deleting data from the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()\n",
    "else:\n",
    "    # Create table if it does not exist\n",
    "    column_text = \", \".join(\n",
    "        f\"{ilincs_2_sql_columns.get(c)} VARCHAR({n + 10})\"\n",
    "        if c not in int_columns\n",
    "        else f\"{ilincs_2_sql_columns.get(c)} INT\"\n",
    "        for c, n in zip(df_data.columns, max_lengths)\n",
    "    )\n",
    "    create_table_query = f\"CREATE TABLE {table_name} ({column_text}, PRIMARY KEY({ilincs_2_sql_columns.get(primary_key)}),FOREIGN KEY ({foreign_key}) REFERENCES {reference_table}({reference_key}));\"\n",
    "    try:\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        logging.info(f\"Table {table_name} created successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while creating the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d8832a-455a-4a3a-9e0b-6378a73a6bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 16:39:36,545 - INFO - Data dumped into signatures successfully.\n"
     ]
    }
   ],
   "source": [
    "# 6. Dump Data into Table\n",
    "data_tuples = list(df_data.itertuples(index=False, name=None))\n",
    "insert_query = (\n",
    "    f\"INSERT INTO {table_name} ({', '.join([ilincs_2_sql_columns.get(c) for c in df_data.columns])}) VALUES (%s\"\n",
    "    + \", %s\" * (len(df_data.columns) - 1)\n",
    "    + \")\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    with conn:\n",
    "        with conn.cursor() as curs:\n",
    "            for record in data_tuples:\n",
    "                try:\n",
    "                    curs.execute(insert_query, record)\n",
    "                except psycopg2.Error as e:\n",
    "                    logging.error(f\"Error inserting record {record}: {e}\")\n",
    "                    # Optionally, you can break the loop after logging the first error\n",
    "                    break\n",
    "    logging.info(f\"Data dumped into {table_name} successfully.\")\n",
    "except psycopg2.Error as e:\n",
    "    logging.error(f\"An error occurred while inserting data into the table: {e}\")\n",
    "    conn.rollback()\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f809d-e8dd-4504-a659-eeb6071e8137",
   "metadata": {},
   "source": [
    "## Create Table Signature Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5675a-0af8-426c-ae0c-945f3a392b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports, Variables, Functions\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import logging, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "\n",
    "# SQL variables\n",
    "dbname = \"ilincs\"\n",
    "user = \"ddalton\"\n",
    "password = \"Teclado$$$111\"\n",
    "host = \"localhost\"\n",
    "path_data = \"../../data/iLINCS/signature_vectors\"\n",
    "table_name = \"signature_values\"\n",
    "primary_key = \"signature_id_gene_id\"\n",
    "int_columns = []  # INT columns - rest TEXT\n",
    "drop_table = True\n",
    "foreign_key = \"signature_id\"\n",
    "reference_table = \"signatures\"\n",
    "reference_key = \"signature_id\"\n",
    "\n",
    "columns_of_interest = [\n",
    "    \"signatureID\",\n",
    "    \"ID_geneid\",\n",
    "    \"Name_GeneSymbol\",\n",
    "    \"Value_LogDiffExp\",\n",
    "    \"Significance_pvalue\",\n",
    "]\n",
    "\n",
    "ilincs_2_sql_columns = {\n",
    "    \"signatureID\": \"signature_id\",\n",
    "    \"ID_geneid\": \"gene_id\",\n",
    "    \"Name_GeneSymbol\": \"gene_name\",\n",
    "    \"Value_LogDiffExp\": \"log_diff_exp\",\n",
    "    \"Significance_pvalue\": \"p_value\",\n",
    "    \"signature_id_gene_id\": \"signature_id_gene_id\",\n",
    "}\n",
    "\n",
    "\n",
    "# functions\n",
    "def get_disease_signatureids():\n",
    "    \"\"\"\n",
    "    Get Disease Datasets\n",
    "    Function to retrieve from those filtered signatures the datasetid\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "    Return:\n",
    "    datasetid: list()\n",
    "        List of unique dataset id's\n",
    "    \"\"\"\n",
    "    path_data = \"../../data/iLINCS/signatures.csv\"\n",
    "    filter_df = lambda df: df[\"libraryid\"] == \"LIB_1\"\n",
    "\n",
    "    # Load Data\n",
    "    try:\n",
    "        df_data = pd.read_csv(path_data)\n",
    "        logging.info(\"Data loaded successfully.\")\n",
    "    except FileNotFoundError:\n",
    "        logging.error(\"Data file not found. Please check the file path.\")\n",
    "        exit()\n",
    "\n",
    "    # filter disease signatures\n",
    "    df_data = df_data[filter_df]\n",
    "\n",
    "    return list(df_data[\"signatureid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data\n",
    "\n",
    "if os.path.exists(\"../../data/iLINCS/disease_signature_vectors.csv\"):\n",
    "    df_data = pd.read_csv(\"../../data/iLINCS/disease_signature_vectors.csv\")\n",
    "else:\n",
    "    # get signature ids\n",
    "    signature_ids = get_disease_signatureids()\n",
    "    loop = 0\n",
    "    # get data\n",
    "    for signature_id in tqdm(signature_ids):\n",
    "        if loop == 0:\n",
    "            try:\n",
    "                df_data = pd.read_csv(os.path.join(path_data, signature_id + \".csv\"))\n",
    "                loop = 1\n",
    "            except FileNotFoundError:\n",
    "                logging.error(\"Data file not found. Please check the file path.\")\n",
    "                exit()\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(path_data, signature_id + \".csv\"))\n",
    "                df_data = pd.concat([df, df_data])\n",
    "            except FileNotFoundError:\n",
    "                logging.error(\"Data file not found. Please check the file path.\")\n",
    "                exit()\n",
    "\n",
    "    # save dataframe\n",
    "    try:\n",
    "        df_data.to_csv(\"../../data/iLINCS/disease_signature_vectors.csv\", index=False)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving data to csv: {e}\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44894a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144545011, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fd0dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create primary key column\n",
    "df_data[\"signature_id_gene_id\"] = (\n",
    "    df_data[\"signatureID\"] + \"_\" + df_data[\"ID_geneid\"].astype(str)\n",
    ")\n",
    "\n",
    "# filter columns of interest\n",
    "df_data = df_data[columns_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a77cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max length for each\n",
    "max_lengths = [\n",
    "    max([len(str(n)) for n in df_data[c].to_list()]) for c in df_data.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "118d871c-64f6-43da-bbb1-f71a8fe8b695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 17:14:16,695 - INFO - Connected to the database successfully.\n",
      "2023-12-14 17:14:16,699 - ERROR - An error occurred while creating the table: column \"signature_id_gene_id\" named in key does not exist\n",
      "LINE 1: ...), log_diff_exp VARCHAR(33), p_value VARCHAR(33), PRIMARY KE...\n",
      "                                                             ^\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table signature_values dropped successfully if it existed.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 3. Connect with Database\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "    logging.info(\"Connected to the database successfully.\")\n",
    "except psycopg2.OperationalError as e:\n",
    "    logging.error(f\"Unable to connect to the database: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 4. Create Cursor Object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# 5. Check if Table Exists and Delete Data if It Does\n",
    "\n",
    "# Check if the table exists and drop it if it does\n",
    "if drop_table:\n",
    "    try:\n",
    "        cursor.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n",
    "        conn.commit()\n",
    "        print(f\"Table {table_name} dropped successfully if it existed.\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "cursor.execute(\n",
    "    \"SELECT EXISTS(SELECT * FROM information_schema.tables WHERE table_name=%s)\",\n",
    "    (table_name,),\n",
    ")\n",
    "table_exists = cursor.fetchone()[0]\n",
    "\n",
    "if table_exists:\n",
    "    try:\n",
    "        cursor.execute(f\"DELETE FROM {table_name};\")\n",
    "        conn.commit()\n",
    "        logging.info(f\"Existing data in table {table_name} deleted successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while deleting data from the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()\n",
    "else:\n",
    "    # Create table if it does not exist\n",
    "    column_text = \", \".join(\n",
    "        f\"{ilincs_2_sql_columns.get(c)} VARCHAR({n + 10})\"\n",
    "        if c not in int_columns\n",
    "        else f\"{ilincs_2_sql_columns.get(c)} INT\"\n",
    "        for c, n in zip(df_data.columns, max_lengths)\n",
    "    )\n",
    "    create_table_query = f\"CREATE TABLE {table_name} ({column_text}, PRIMARY KEY({ilincs_2_sql_columns.get(primary_key)}),FOREIGN KEY ({foreign_key}) REFERENCES {reference_table}({reference_key}));\"\n",
    "    try:\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        logging.info(f\"Table {table_name} created successfully.\")\n",
    "    except psycopg2.Error as e:\n",
    "        logging.error(f\"An error occurred while creating the table: {e}\")\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec70d944-8a7f-480e-a07a-6e78b8f13fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_810975/3237752548.py:17: DtypeWarning: Columns (0,3,7,8,9,10,15,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_data = pd.read_csv(path_data)\n",
      "2023-12-13 18:25:11,755 - INFO - Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "signature_ids = get_disease_signatureids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae734bb8-d89a-4fed-b132-6f19511a57fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signatureID</th>\n",
       "      <th>PROBE</th>\n",
       "      <th>ID_geneid</th>\n",
       "      <th>Name_GeneSymbol</th>\n",
       "      <th>Value_LogDiffExp</th>\n",
       "      <th>Significance_pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GDS_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5320</td>\n",
       "      <td>PLA2G2A</td>\n",
       "      <td>-8.002820</td>\n",
       "      <td>5.012735e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GDS_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5321</td>\n",
       "      <td>PLA2G4A</td>\n",
       "      <td>7.830690</td>\n",
       "      <td>7.822211e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GDS_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92235</td>\n",
       "      <td>DUSP27</td>\n",
       "      <td>-9.207620</td>\n",
       "      <td>1.222107e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GDS_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9324</td>\n",
       "      <td>HMGN3</td>\n",
       "      <td>7.586360</td>\n",
       "      <td>2.091724e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GDS_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2697</td>\n",
       "      <td>GJA1</td>\n",
       "      <td>7.700330</td>\n",
       "      <td>2.495001e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19680</th>\n",
       "      <td>GDS_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>388419</td>\n",
       "      <td>BTBD17</td>\n",
       "      <td>0.000170</td>\n",
       "      <td>9.989195e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19681</th>\n",
       "      <td>GDS_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10664</td>\n",
       "      <td>CTCF</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>9.990614e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19682</th>\n",
       "      <td>GDS_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150379</td>\n",
       "      <td>PNPLA5</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>9.991050e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19683</th>\n",
       "      <td>GDS_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56901</td>\n",
       "      <td>NDUFA4L2</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>9.992070e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19684</th>\n",
       "      <td>GDS_1000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>255512</td>\n",
       "      <td>TOLLIP-AS1</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>9.998537e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19685 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      signatureID  PROBE  ID_geneid Name_GeneSymbol  Value_LogDiffExp  \\\n",
       "0        GDS_1000    NaN       5320         PLA2G2A         -8.002820   \n",
       "1        GDS_1000    NaN       5321         PLA2G4A          7.830690   \n",
       "2        GDS_1000    NaN      92235          DUSP27         -9.207620   \n",
       "3        GDS_1000    NaN       9324           HMGN3          7.586360   \n",
       "4        GDS_1000    NaN       2697            GJA1          7.700330   \n",
       "...           ...    ...        ...             ...               ...   \n",
       "19680    GDS_1000    NaN     388419          BTBD17          0.000170   \n",
       "19681    GDS_1000    NaN      10664            CTCF         -0.000133   \n",
       "19682    GDS_1000    NaN     150379          PNPLA5          0.000153   \n",
       "19683    GDS_1000    NaN      56901        NDUFA4L2          0.000107   \n",
       "19684    GDS_1000    NaN     255512      TOLLIP-AS1          0.000023   \n",
       "\n",
       "       Significance_pvalue  \n",
       "0             5.012735e-14  \n",
       "1             7.822211e-14  \n",
       "2             1.222107e-13  \n",
       "3             2.091724e-13  \n",
       "4             2.495001e-13  \n",
       "...                    ...  \n",
       "19680         9.989195e-01  \n",
       "19681         9.990614e-01  \n",
       "19682         9.991050e-01  \n",
       "19683         9.992070e-01  \n",
       "19684         9.998537e-01  \n",
       "\n",
       "[19685 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/iLINCS/signature_vectors/GDS_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb018c5-e13a-45bf-9fbc-a8da3bc52a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_columns = [\"ID_geneid\"]\n",
    "float_columns = [\"Value_LogDiffExp\", \"Significance_pvalue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10f9085a-eba8-4d7c-a8f4-b4d009202080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235982"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_data[\"geneid\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2f16b1-2756-4b04-9080-33aa156cac6d",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e67400-3308-4a6f-aa1b-3c44a73513ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fafed3de-1966-4f52-aaac-bcc0a09055ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully.\n",
      "     signature_id  dataset_id condition_1                     condition_2  \\\n",
      "0           GDS_1      GDS690      Caco-2                             T84   \n",
      "1          GDS_10      GDS852     control                         stretch   \n",
      "2         GDS_100  gdsGDS1454          6q  unknown_chromosomal_aberration   \n",
      "3        GDS_1000  gdsGDS4296        A549                         COLO205   \n",
      "4        GDS_1001  gdsGDS4296        A549                     DU_145(DTP)   \n",
      "...           ...         ...         ...                             ...   \n",
      "9092      GDS_994  gdsGDS4296        A498                        UACC_257   \n",
      "9093      GDS_995  gdsGDS4296        A498                         UACC_62   \n",
      "9094      GDS_996  gdsGDS4296        A498                           UO_31   \n",
      "9095      GDS_997  gdsGDS4296        A549                            ACHN   \n",
      "9096      GDS_999  gdsGDS4296        A549                        CCRF_CEM   \n",
      "\n",
      "     tissue cell_line  \n",
      "0      None      None  \n",
      "1      None      None  \n",
      "2      None      None  \n",
      "3      None      None  \n",
      "4      None      None  \n",
      "...     ...       ...  \n",
      "9092   None      None  \n",
      "9093   None      None  \n",
      "9094   None      None  \n",
      "9095   None      None  \n",
      "9096   None      None  \n",
      "\n",
      "[9097 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_810975/183834926.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    }
   ],
   "source": [
    "# SQL variables\n",
    "dbname = \"ilincs\"\n",
    "user = \"ddalton\"\n",
    "password = \"Teclado$$$111\"\n",
    "host = \"localhost\"\n",
    "table_name = \"signatures\"\n",
    "columns_of_interest = {\"ID_genid\":\"gene_id\",\n",
    "                       \"\",\n",
    "                       \"\"}\n",
    "# Connect to the database\n",
    "try:\n",
    "    conn = psycopg2.connect(dbname=dbname, user=user, password=password, host=host)\n",
    "    print(\"Connected to the database successfully.\")\n",
    "except psycopg2.OperationalError as e:\n",
    "    print(f\"Unable to connect to the database: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Query the table\n",
    "try:\n",
    "    query = f\"SELECT * FROM {table_name} LIMIT 10;\"  # Adjust the query as needed\n",
    "    query = f\"SELECT * FROM {table_name} WHERE antibodytarget IS NOT NULL;\"\n",
    "    query = f\"SELECT * FROM {table_name} WHERE experiment IS  NULL;\"\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    print(df)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while querying the table: {e}\")\n",
    "finally:\n",
    "    conn.close()\n",
    "\n",
    "# The DataFrame 'df' now contains the first 10 rows of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d69e7-060b-4831-b3c9-88463ea1a8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
